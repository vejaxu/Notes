# 《机器学习》-ed1-周志华-学习笔记

## ch4-决策树

一颗决策树包含一个根结点，若干个内部结点，若干个叶结点。叶结点对应于决策结果，其他每个结点对应于一个属性测试。

决策树生成是一个递归的过程：

- 当前结点包含的样本全属于同一个类被，无需划分
- 当前属性集为空，或者所有样本在所有属性上取值相同，无法划分——当前结点标记为叶结点
- 当前结点包含的样本集合为空，不能划分

### 划分选择

信息熵（information entropy）是度量样本集合纯度最常用的一种指标，对于样本集合D中第k类样本所占比例为$p_k$ ，样本集合D的信息熵为：
$$
Ent\left(D\right) = - \sum_{i=1}^{|y|} p_k \log_2 p_k
$$
熵值越少，纯度越高。

假定离散属性 $\alpha$ 有V个可能的取值$\lbrace\alpha^1, \alpha^2, ..., \alpha^V \rbrace$ ，若使用$\alpha$​ 对样本D进行划分，产生V个分支结点。

- 信息增益（information gain）：

$$
Gain\left(D, \alpha \right) = Ent\left(D\right) - \sum_{v=1}^{V} \frac{|D^v|}{|D|}Ent(D^v)
$$

信息增益越大，属性 $\alpha$ 对样本集合划分所获得的纯度提升越大。

- 增益率（gain ratio）：

$$
Gain_ratio(D, \alpha) = \frac {Gain(D, \alpha)}{IV(\alpha)} \\
IV(\alpha) = - \sum _{v=1}^{V} \frac{|D_v|}{|D|} \log_2{\frac{|D_v|}{|D|}}
$$

其中，$IV(\alpha)$ 被称为属性 $\alpha$​ 的固有值。

- Gini指数

$$
Gini(D) = 1 - \sum_{k=1}^{|y|}p_k^2 \\
Gini\_index(D, \alpha) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} \cdot Gini\left(D^v\right)
$$

Gini指数反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率，指数越小，纯度越高。

### 剪枝处理

剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段。

基本策略为预剪枝和后剪枝

- 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。
- 后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点对应的子树替换为叶结点能够带来决策树泛化性能的提升，则将该子树替换为叶结点。

判断泛化性能提升的方法：使用留出法，预留一部分数据用作验证集进行性能评估。

### 连续与缺失值

当属性为连续值，可以使用连续属性离散化。在C4.5决策树算法中，采用二分法（bi-partition）对连续属性进行处理。

给定样本集D以及连续属性 $\alpha$ ，假定 $\alpha$ 在D上出现n个不同的取值，将n个值进行大小排序 $\lbrace\alpha^1, \alpha^2, ..., \alpha^n \rbrace$ 。基于划分点t可将D分为子集 $D_t^-$ 以及 $D_t^+$ 。其中左侧子集包含属性 $\alpha$ 取值不大于t的样本，右侧子集包含取值大于t的样本。

则候选划分点集合为
$$
T_\alpha = \lbrace \frac{\alpha^i + \alpha^{i + 1}}{2} | 1 \leq i \leq n-1 \rbrace
$$
则候选划分点集合有n-1个元素。

对每个候选点，其次计算二分后信息增益，选取信息增益最大的候选点作为划分点。

当样本的某些属性值缺失时，需要解决两个问题：

- 如何在属性值缺失的情况下进行划分属性选择？
- 给定划分属性，若样本在该属性上的值缺失，如何对该样本进行划分？

### 多变量决策树

将每个属性是为坐标空间中的一个坐标轴，则d个属性描述的样本对应d维空间中的一个数据点，对样本分类即表示在这个坐标空间寻找不同类样本之间的分类边界。

决策树形成的分类边界有一个明显的特点：轴平行。即分类边界由若干个与坐标轴平行的分段组成。

分类边界的每一段都与坐标轴平行，这样的分类边界使得学习结果有较好的可解释性，但当真是分类边界比较复杂时，必须使用很多段划分才能获得较好的近似。

而若使用斜的划分边界，模型得到大幅简化。多变量决策树实现斜的划分。

### 小结

决策树算法：ID3，C4.5，CART

特征选择：信息增益，增益率，基尼指数

多变量决策树：OC1——贪心地寻找每个属性的最优权值

增量学习（incremental learning）：接收到新样本后对已学得的模型进行调整